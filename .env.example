# Model Configuration
MODEL_NAME=meta-llama/Llama-3.1-70B-Instruct
TENSOR_PARALLEL=1
GPU_MEMORY_UTIL=0.95
MAX_MODEL_LEN=32768

# Benchmark Configuration
CONFIG_PATH=/configs/llama3_70b_sweep.yaml

# Optional: HuggingFace token for gated models
# HF_TOKEN=your_token_here
