# Context Window & KV Cache Sweep
# Tests model performance across different context lengths

benchmark:
  name: "Llama 3.1 70B Context Window Sweep"
  description: "Measure TTFT, ITL, and VRAM usage across context lengths"
  output_prefix: "llama3_70b_sweep"

model:
  name: "meta-llama/Llama-3.1-70B-Instruct"
  quantization: "fp8"  # or "awq", "none"
  tensor_parallel_size: 1
  enable_prefix_caching: true

# Context length sweep
context_lengths:
  - 8192
  - 32768
  - 65536
  - 98304
  - 131072

# Request configuration
requests:
  num_requests: 100
  concurrent_requests: 10
  arrival_pattern: "poisson"  # or "burst", "constant"
  rate_per_second: 2.0
  
  # Prompt generation
  prompt_tokens_min: 512
  prompt_tokens_max: 2048
  completion_tokens: 512
  
  # System prompt for realistic workload
  system_prompt: |
    You are a helpful AI assistant. Provide detailed and accurate responses.

# Metrics to collect
metrics:
  - ttft  # Time to First Token
  - itl   # Inter-Token Latency
  - throughput  # Tokens per second
  - memory_usage  # VRAM consumption
  - kv_cache_usage  # KV cache memory

# Telemetry sampling
telemetry:
  sample_interval_sec: 1.0
  collect_gpu_stats: true
  collect_power_draw: true
  collect_temperature: true
  collect_memory_clock: true

# Performance targets (for reporting)
targets:
  ttft_ms: 200
  itl_ms: 50
  throughput_tokens_per_sec: 1000
