# Multi-Tenancy Simulation Test
# Tests concurrent model serving scenarios

benchmark:
  name: "Multi-Tenancy Performance Test"
  description: "Evaluate shared vs partitioned VRAM allocation strategies"
  output_prefix: "multitenancy_test"

# Scenario A: Shared Engine with Multi-LoRA
scenario_a:
  enabled: true
  name: "Shared Large Model with LoRA Adapters"
  
  base_model:
    name: "meta-llama/Llama-3.1-70B-Instruct"
    quantization: "fp8"
    tensor_parallel_size: 1
  
  lora_adapters:
    - name: "tenant_1_customer_support"
      path: ""  # Optional: path to LoRA weights
      allocation_weight: 0.4  # 40% of requests
    - name: "tenant_2_code_assistant"
      path: ""
      allocation_weight: 0.3  # 30% of requests
    - name: "tenant_3_general"
      path: ""
      allocation_weight: 0.3  # 30% of requests
  
  requests:
    total_requests: 300
    concurrent_requests: 15
    arrival_pattern: "poisson"
    rate_per_second: 5.0
    prompt_tokens_range: [256, 1024]
    completion_tokens: 256

# Scenario B: Partitioned VRAM
scenario_b:
  enabled: true
  name: "Partitioned Models with Hard VRAM Limits"
  description: "1x 48GB model + 2x 24GB models"
  
  # Note: This requires multiple docker-compose services with CUDA_VISIBLE_DEVICES set
  partitions:
    - name: "large_model"
      model: "meta-llama/Llama-3.1-70B-Instruct"
      vram_limit_gb: 48
      quantization: "awq"
      port: 8000
      request_allocation: 0.5  # 50% of total requests
      
    - name: "medium_model_1"
      model: "meta-llama/Llama-3.1-8B-Instruct"
      vram_limit_gb: 24
      quantization: "none"
      port: 8001
      request_allocation: 0.25  # 25% of requests
      
    - name: "medium_model_2"
      model: "mistralai/Mistral-7B-Instruct-v0.3"
      vram_limit_gb: 24
      quantization: "none"
      port: 8002
      request_allocation: 0.25  # 25% of requests
  
  requests:
    total_requests: 300
    concurrent_requests_per_partition: 5
    arrival_pattern: "burst"
    prompt_tokens_range: [256, 1024]
    completion_tokens: 256

# Common metrics for both scenarios
metrics:
  - ttft
  - itl
  - throughput
  - memory_usage
  - gpu_utilization
  - request_latency_p50
  - request_latency_p95
  - request_latency_p99

# Telemetry
telemetry:
  sample_interval_sec: 0.5
  collect_gpu_stats: true
  collect_per_tenant_metrics: true

# Analysis targets
targets:
  max_ttft_ms: 300
  max_itl_ms: 75
  min_throughput_tokens_per_sec: 800
  max_vram_utilization: 0.95
