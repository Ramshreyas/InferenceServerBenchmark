# Quick Sanity Check
# Fast validation test to ensure the setup is working

benchmark:
  name: "Quick Sanity Check"
  description: "Fast test to validate vLLM server and benchmarking setup"
  output_prefix: "sanity_check"

model:
  name: "meta-llama/Llama-3.1-8B-Instruct"  # Smaller model for quick testing
  quantization: "none"
  tensor_parallel_size: 1
  enable_prefix_caching: false

context_lengths:
  - 8192

requests:
  num_requests: 10
  concurrent_requests: 2
  arrival_pattern: "constant"
  rate_per_second: 1.0
  
  prompt_tokens_min: 128
  prompt_tokens_max: 256
  completion_tokens: 128
  
  system_prompt: |
    You are a helpful AI assistant.

metrics:
  - ttft
  - itl
  - throughput
  - memory_usage

telemetry:
  sample_interval_sec: 2.0
  collect_gpu_stats: true
  collect_power_draw: false
  collect_temperature: false
  collect_memory_clock: false

targets:
  ttft_ms: 500
  itl_ms: 100
  throughput_tokens_per_sec: 100
