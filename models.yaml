# =============================================================================
# models.yaml  —  THE ONLY FILE YOU NEED TO EDIT
# =============================================================================
# List every model you want to benchmark. `make sweep`, `make kv-analysis`
# and `make multitenancy-shared` will iterate over this list automatically.
#
# Fields per model:
#   name            HuggingFace model ID
#   label           Short slug used in output filenames (no spaces)
#   quantization    none | fp8 | awq | gptq
#   max_model_len   Maximum context length in tokens (must fit in VRAM)
#                   This is a hard ceiling enforced by vLLM at startup — any
#                   benchmark request exceeding this length will be rejected.
#                   Set it to the highest context_sweep.yaml sweep point that
#                   fits in remaining VRAM after model weights are loaded.
#   gpu_memory_util Fraction of GPU VRAM to allocate (0.0 – 1.0)
#   tensor_parallel Number of GPUs to shard across (1 for single-GPU)
#   topology        dense | sparse_moe
#                   dense:      all parameters are active on every forward pass.
#                   sparse_moe: Mixture-of-Experts — only a small subset of
#                               parameters (the "active" params) are used per
#                               token, but ALL expert weights still load into
#                               VRAM. VRAM requirement is set by total param
#                               count, not active param count.
#
# All models below are publicly available without gated access.
# =============================================================================

models:

  # ── Large (70B–120B) ─────────────────────────────────────────────────────────
  # Dense: all weights active. Sparse MoE: all weights load into VRAM regardless
  # of active-param count — VRAM requirement is determined by total param count.

  - name: openai/gpt-oss-120b
    label: gpt-oss-120b
    quantization: none          # Pre-quantized mxfp4 (Blackwell-native 4-bit); ~60 GB loaded
    max_model_len: 32768        # ~36 GB KV headroom → covers 8k/32k sweep points
    gpu_memory_util: 0.95
    tensor_parallel: 1
    topology: dense

  - name: deepseek-ai/DeepSeek-R1-Distill-Llama-70B
    label: deepseek-r1-llama-70b
    quantization: fp8           # FP8 ~72 GB loaded
    max_model_len: 32768        # ~24 GB KV headroom → covers 8k/32k sweep points
    gpu_memory_util: 0.95
    tensor_parallel: 1
    topology: dense

  - name: Qwen/Qwen3-Next-80B-A3B-Thinking
    label: qwen3-next-80b-thinking
    quantization: fp8           # BF16 ~160 GB – FP8 brings it to ~80 GB; fits with tight headroom
    max_model_len: 16384        # ~16 GB KV headroom → covers 8k/16k sweep points only
    gpu_memory_util: 0.95
    tensor_parallel: 1
    topology: sparse_moe        # 80B total weights, 3B active per forward pass

  # ── Medium (30B–32B) ─────────────────────────────────────────────────────────

  - name: Qwen/Qwen3-30B-A3B-Thinking-2507
    label: qwen3-30b-thinking
    quantization: none          # BF16 ~60 GB loaded (MoE: small active KV footprint)
    max_model_len: 65536        # ~36 GB KV headroom → covers 8k/32k/65k sweep points
    gpu_memory_util: 0.92
    tensor_parallel: 1
    topology: sparse_moe        # 30B total weights, 3B active per forward pass

  - name: Qwen/QwQ-32B
    label: qwq-32b
    quantization: fp8           # FP8 ~33 GB loaded
    max_model_len: 65536        # ~63 GB KV headroom → covers 8k/32k/65k sweep points
    gpu_memory_util: 0.90
    tensor_parallel: 1
    topology: dense

  - name: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
    label: deepseek-r1-qwen-32b
    quantization: fp8           # FP8 ~33 GB loaded
    max_model_len: 65536        # ~63 GB KV headroom → covers 8k/32k/65k sweep points
    gpu_memory_util: 0.90
    tensor_parallel: 1
    topology: dense

  # ── Small (7B–8B) ────────────────────────────────────────────────────────────

  - name: Qwen/Qwen3-8B
    label: qwen3-8b
    quantization: none          # BF16 ~16 GB loaded
    max_model_len: 131072       # ~80 GB KV headroom → covers all sweep points incl. 131k
    gpu_memory_util: 0.90
    tensor_parallel: 1
    topology: dense

  - name: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
    label: deepseek-r1-qwen-7b
    quantization: none          # BF16 ~15 GB loaded
    max_model_len: 131072       # ~81 GB KV headroom → covers all sweep points incl. 131k
    gpu_memory_util: 0.90
    tensor_parallel: 1
    topology: dense

  - name: mistralai/Mistral-7B-Instruct-v0.3
    label: mistral-7b
    quantization: none          # BF16 ~15 GB loaded
    max_model_len: 131072       # ~81 GB KV headroom → covers all sweep points incl. 131k
    gpu_memory_util: 0.90
    tensor_parallel: 1
    topology: dense
