{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0723f7ac",
   "metadata": {},
   "source": [
    "# Blackwell Inference Benchmark Analysis\n",
    "\n",
    "This notebook analyzes benchmark results from the vLLM inference testbench.\n",
    "\n",
    "**Metrics analyzed:**\n",
    "- Time to First Token (TTFT)\n",
    "- Inter-Token Latency (ITL)\n",
    "- Throughput (tokens/sec)\n",
    "- GPU Telemetry (VRAM, power, temperature)\n",
    "\n",
    "**Usage:**\n",
    "1. Run benchmarks on the server\n",
    "2. Copy results to local `../results/` directory\n",
    "3. Update the `RESULT_FILE` path below\n",
    "4. Run all cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3998a3a",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f8bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Styling\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2021b5c7",
   "metadata": {},
   "source": [
    "## Load Benchmark Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f34139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths\n",
    "RESULTS_DIR = Path('../results')\n",
    "\n",
    "# List available result files\n",
    "json_files = sorted(RESULTS_DIR.glob('*_detailed.json'))\n",
    "print(f\"Found {len(json_files)} result files:\\n\")\n",
    "for i, f in enumerate(json_files):\n",
    "    print(f\"{i}: {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f24527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select result file (change index or filename)\n",
    "RESULT_FILE = json_files[-1] if json_files else None  # Most recent file\n",
    "\n",
    "if RESULT_FILE is None:\n",
    "    raise FileNotFoundError(\"No result files found in ../results/\")\n",
    "\n",
    "print(f\"Loading: {RESULT_FILE.name}\")\n",
    "\n",
    "# Load JSON data\n",
    "with open(RESULT_FILE, 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "print(f\"\\n✓ Loaded {len(df)} requests\")\n",
    "print(f\"  Successful: {df['success'].sum()}\")\n",
    "print(f\"  Failed: {(~df['success']).sum()}\")\n",
    "\n",
    "# Filter successful requests only\n",
    "df_success = df[df['success']].copy()\n",
    "\n",
    "# Display sample\n",
    "df_success.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef14bdd",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd159cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"  Total requests: {len(df_success)}\")\n",
    "print(f\"  Context lengths: {sorted(df_success['context_length'].unique())}\")\n",
    "print(f\"  Prompt tokens: {df_success['prompt_tokens'].min()}-{df_success['prompt_tokens'].max()}\")\n",
    "print(f\"  Tokens generated: {df_success['tokens_generated'].sum()}\")\n",
    "\n",
    "# Basic statistics\n",
    "df_success[['ttft_ms', 'itl_ms', 'throughput_tokens_per_sec', 'total_latency_ms']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb4bbf",
   "metadata": {},
   "source": [
    "## Time to First Token (TTFT) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede0e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTFT distribution\n",
    "fig = px.histogram(\n",
    "    df_success,\n",
    "    x='ttft_ms',\n",
    "    nbins=50,\n",
    "    title='Time to First Token Distribution',\n",
    "    labels={'ttft_ms': 'TTFT (ms)', 'count': 'Frequency'},\n",
    "    marginal='box'\n",
    ")\n",
    "fig.update_layout(showlegend=False, height=500)\n",
    "fig.show()\n",
    "\n",
    "# Statistics\n",
    "print(\"\\nTTFT Statistics:\")\n",
    "print(f\"  Mean: {df_success['ttft_ms'].mean():.2f} ms\")\n",
    "print(f\"  Median (P50): {df_success['ttft_ms'].median():.2f} ms\")\n",
    "print(f\"  P95: {df_success['ttft_ms'].quantile(0.95):.2f} ms\")\n",
    "print(f\"  P99: {df_success['ttft_ms'].quantile(0.99):.2f} ms\")\n",
    "print(f\"  Max: {df_success['ttft_ms'].max():.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae456a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTFT by context length\n",
    "if 'context_length' in df_success.columns:\n",
    "    fig = px.box(\n",
    "        df_success,\n",
    "        x='context_length',\n",
    "        y='ttft_ms',\n",
    "        title='TTFT Distribution by Context Length',\n",
    "        labels={'context_length': 'Context Length', 'ttft_ms': 'TTFT (ms)'},\n",
    "        points='outliers'\n",
    "    )\n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6716a7f",
   "metadata": {},
   "source": [
    "## Inter-Token Latency (ITL) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe68b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITL distribution\n",
    "fig = px.histogram(\n",
    "    df_success,\n",
    "    x='itl_ms',\n",
    "    nbins=50,\n",
    "    title='Inter-Token Latency Distribution',\n",
    "    labels={'itl_ms': 'ITL (ms)', 'count': 'Frequency'},\n",
    "    marginal='box'\n",
    ")\n",
    "fig.update_layout(showlegend=False, height=500)\n",
    "fig.show()\n",
    "\n",
    "# Statistics\n",
    "print(\"\\nITL Statistics:\")\n",
    "print(f\"  Mean: {df_success['itl_ms'].mean():.2f} ms\")\n",
    "print(f\"  Median (P50): {df_success['itl_ms'].median():.2f} ms\")\n",
    "print(f\"  P95: {df_success['itl_ms'].quantile(0.95):.2f} ms\")\n",
    "print(f\"  Min: {df_success['itl_ms'].min():.2f} ms\")\n",
    "print(f\"  Max: {df_success['itl_ms'].max():.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb3cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITL over time (request sequence)\n",
    "fig = px.scatter(\n",
    "    df_success,\n",
    "    x='request_id',\n",
    "    y='itl_ms',\n",
    "    color='context_length' if 'context_length' in df_success.columns else None,\n",
    "    title='Inter-Token Latency Over Request Sequence',\n",
    "    labels={'request_id': 'Request ID', 'itl_ms': 'ITL (ms)'},\n",
    "    opacity=0.6\n",
    ")\n",
    "fig.update_layout(height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab47b9",
   "metadata": {},
   "source": [
    "## Throughput Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffb7978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throughput distribution\n",
    "fig = px.histogram(\n",
    "    df_success,\n",
    "    x='throughput_tokens_per_sec',\n",
    "    nbins=50,\n",
    "    title='Throughput Distribution',\n",
    "    labels={'throughput_tokens_per_sec': 'Throughput (tokens/sec)', 'count': 'Frequency'},\n",
    "    marginal='box'\n",
    ")\n",
    "fig.update_layout(showlegend=False, height=500)\n",
    "fig.show()\n",
    "\n",
    "# Statistics\n",
    "print(\"\\nThroughput Statistics:\")\n",
    "print(f\"  Mean: {df_success['throughput_tokens_per_sec'].mean():.2f} tokens/sec\")\n",
    "print(f\"  Median: {df_success['throughput_tokens_per_sec'].median():.2f} tokens/sec\")\n",
    "print(f\"  Total tokens generated: {df_success['tokens_generated'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d8b4b",
   "metadata": {},
   "source": [
    "## Context Length Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9be43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'context_length' in df_success.columns:\n",
    "    # Group by context length\n",
    "    ctx_summary = df_success.groupby('context_length').agg({\n",
    "        'ttft_ms': ['mean', 'median', lambda x: x.quantile(0.95)],\n",
    "        'itl_ms': ['mean', 'median', lambda x: x.quantile(0.95)],\n",
    "        'throughput_tokens_per_sec': ['mean', 'median'],\n",
    "        'total_latency_ms': ['mean', 'median'],\n",
    "        'request_id': 'count'\n",
    "    }).round(2)\n",
    "    \n",
    "    ctx_summary.columns = ['_'.join(col).strip() for col in ctx_summary.columns.values]\n",
    "    ctx_summary.rename(columns={'request_id_count': 'num_requests'}, inplace=True)\n",
    "    \n",
    "    print(\"\\nPerformance by Context Length:\")\n",
    "    display(ctx_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af1e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-metric comparison across context lengths\n",
    "if 'context_length' in df_success.columns:\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('TTFT by Context Length', 'ITL by Context Length',\n",
    "                       'Throughput by Context Length', 'Total Latency by Context Length')\n",
    "    )\n",
    "    \n",
    "    metrics = [\n",
    "        ('ttft_ms', 'TTFT (ms)'),\n",
    "        ('itl_ms', 'ITL (ms)'),\n",
    "        ('throughput_tokens_per_sec', 'Throughput (tokens/sec)'),\n",
    "        ('total_latency_ms', 'Total Latency (ms)')\n",
    "    ]\n",
    "    \n",
    "    positions = [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
    "    \n",
    "    for (metric, label), (row, col) in zip(metrics, positions):\n",
    "        for ctx_len in sorted(df_success['context_length'].unique()):\n",
    "            data = df_success[df_success['context_length'] == ctx_len][metric]\n",
    "            fig.add_trace(\n",
    "                go.Box(y=data, name=str(ctx_len), showlegend=(row==1 and col==1)),\n",
    "                row=row, col=col\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(height=800, title_text=\"Performance Metrics Across Context Lengths\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda25d08",
   "metadata": {},
   "source": [
    "## GPU Telemetry Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2400f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load telemetry data if available\n",
    "telemetry_file = RESULT_FILE.parent / RESULT_FILE.name.replace('_detailed.json', '_telemetry.json')\n",
    "\n",
    "if telemetry_file.exists():\n",
    "    print(f\"Loading telemetry: {telemetry_file.name}\")\n",
    "    \n",
    "    with open(telemetry_file, 'r') as f:\n",
    "        telemetry = json.load(f)\n",
    "    \n",
    "    df_telem = pd.DataFrame(telemetry)\n",
    "    \n",
    "    # Filter out error entries\n",
    "    df_telem = df_telem[~df_telem['timestamp'].isna()].copy()\n",
    "    \n",
    "    if len(df_telem) > 0:\n",
    "        # Convert timestamp\n",
    "        df_telem['timestamp'] = pd.to_datetime(df_telem['timestamp'])\n",
    "        df_telem['elapsed_sec'] = (df_telem['timestamp'] - df_telem['timestamp'].min()).dt.total_seconds()\n",
    "        \n",
    "        print(f\"\\n✓ Loaded {len(df_telem)} telemetry samples\")\n",
    "        print(f\"  Duration: {df_telem['elapsed_sec'].max():.1f} seconds\")\n",
    "        \n",
    "        # Display sample\n",
    "        display(df_telem.head())\n",
    "    else:\n",
    "        print(\"No valid telemetry data found\")\n",
    "        df_telem = None\n",
    "else:\n",
    "    print(\"No telemetry file found\")\n",
    "    df_telem = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a040b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU metrics over time\n",
    "if df_telem is not None and len(df_telem) > 0:\n",
    "    fig = make_subplots(\n",
    "        rows=4, cols=1,\n",
    "        subplot_titles=('GPU Utilization', 'VRAM Usage', 'Temperature', 'Power Draw'),\n",
    "        vertical_spacing=0.08\n",
    "    )\n",
    "    \n",
    "    # GPU Utilization\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_telem['elapsed_sec'], y=df_telem['gpu_utilization_percent'],\n",
    "                  mode='lines', name='GPU Util %', line=dict(color='blue')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # VRAM Usage\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_telem['elapsed_sec'], y=df_telem['memory_used_mb']/1024,\n",
    "                  mode='lines', name='VRAM (GB)', line=dict(color='green')),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Temperature\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_telem['elapsed_sec'], y=df_telem['temperature_c'],\n",
    "                  mode='lines', name='Temp (C)', line=dict(color='red')),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # Power Draw\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_telem['elapsed_sec'], y=df_telem['power_draw_w'],\n",
    "                  mode='lines', name='Power (W)', line=dict(color='orange')),\n",
    "        row=4, col=1\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Time (seconds)\", row=4, col=1)\n",
    "    fig.update_yaxes(title_text=\"%\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"GB\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"°C\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Watts\", row=4, col=1)\n",
    "    \n",
    "    fig.update_layout(height=1000, title_text=\"GPU Telemetry Over Time\", showlegend=False)\n",
    "    fig.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nGPU Telemetry Summary:\")\n",
    "    print(f\"  Peak VRAM: {df_telem['memory_used_mb'].max()/1024:.2f} GB\")\n",
    "    print(f\"  Avg GPU Util: {df_telem['gpu_utilization_percent'].mean():.1f}%\")\n",
    "    print(f\"  Peak Temperature: {df_telem['temperature_c'].max():.1f}°C\")\n",
    "    print(f\"  Avg Power Draw: {df_telem['power_draw_w'].mean():.1f}W\")\n",
    "    print(f\"  Peak Power Draw: {df_telem['power_draw_w'].max():.1f}W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457f8d41",
   "metadata": {},
   "source": [
    "## Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac762102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"=\"*60)\n",
    "print(\"BENCHMARK SUMMARY REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFile: {RESULT_FILE.name}\")\n",
    "print(f\"Total Requests: {len(df)}\")\n",
    "print(f\"Successful: {len(df_success)} ({len(df_success)/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n--- TIME TO FIRST TOKEN (TTFT) ---\")\n",
    "print(f\"Mean:   {df_success['ttft_ms'].mean():.2f} ms\")\n",
    "print(f\"Median: {df_success['ttft_ms'].median():.2f} ms\")\n",
    "print(f\"P95:    {df_success['ttft_ms'].quantile(0.95):.2f} ms\")\n",
    "print(f\"P99:    {df_success['ttft_ms'].quantile(0.99):.2f} ms\")\n",
    "\n",
    "print(\"\\n--- INTER-TOKEN LATENCY (ITL) ---\")\n",
    "print(f\"Mean:   {df_success['itl_ms'].mean():.2f} ms\")\n",
    "print(f\"Median: {df_success['itl_ms'].median():.2f} ms\")\n",
    "print(f\"P95:    {df_success['itl_ms'].quantile(0.95):.2f} ms\")\n",
    "\n",
    "print(\"\\n--- THROUGHPUT ---\")\n",
    "print(f\"Mean:   {df_success['throughput_tokens_per_sec'].mean():.2f} tokens/sec\")\n",
    "print(f\"Median: {df_success['throughput_tokens_per_sec'].median():.2f} tokens/sec\")\n",
    "print(f\"Total tokens: {df_success['tokens_generated'].sum()}\")\n",
    "\n",
    "if df_telem is not None and len(df_telem) > 0:\n",
    "    print(\"\\n--- GPU TELEMETRY ---\")\n",
    "    print(f\"Peak VRAM:      {df_telem['memory_used_mb'].max()/1024:.2f} GB\")\n",
    "    print(f\"Avg GPU Util:   {df_telem['gpu_utilization_percent'].mean():.1f}%\")\n",
    "    print(f\"Peak Temp:      {df_telem['temperature_c'].max():.1f}°C\")\n",
    "    print(f\"Avg Power:      {df_telem['power_draw_w'].mean():.1f}W\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa1df5",
   "metadata": {},
   "source": [
    "## Compare Multiple Benchmark Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57322fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare multiple result files\n",
    "def load_benchmark(file_path):\n",
    "    \"\"\"Load a benchmark result file and return summary statistics.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df_success = df[df['success']].copy()\n",
    "    \n",
    "    return {\n",
    "        'filename': file_path.name,\n",
    "        'total_requests': len(df),\n",
    "        'successful': len(df_success),\n",
    "        'ttft_mean': df_success['ttft_ms'].mean(),\n",
    "        'ttft_p95': df_success['ttft_ms'].quantile(0.95),\n",
    "        'itl_mean': df_success['itl_ms'].mean(),\n",
    "        'itl_p95': df_success['itl_ms'].quantile(0.95),\n",
    "        'throughput_mean': df_success['throughput_tokens_per_sec'].mean(),\n",
    "        'total_tokens': df_success['tokens_generated'].sum()\n",
    "    }\n",
    "\n",
    "# Load all available result files\n",
    "if len(json_files) > 1:\n",
    "    comparison_data = [load_benchmark(f) for f in json_files[-5:]]  # Last 5 runs\n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"\\nComparison of Recent Benchmark Runs:\")\n",
    "    display(df_comparison.round(2))\n",
    "else:\n",
    "    print(\"Only one benchmark result available. Run more benchmarks to enable comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591205cc",
   "metadata": {},
   "source": [
    "## Export Summary\n",
    "\n",
    "Export key metrics to a summary file for tracking over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793976aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary export\n",
    "summary = {\n",
    "    'benchmark_file': RESULT_FILE.name,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'total_requests': len(df),\n",
    "    'successful_requests': len(df_success),\n",
    "    'success_rate': len(df_success) / len(df),\n",
    "    'ttft': {\n",
    "        'mean_ms': float(df_success['ttft_ms'].mean()),\n",
    "        'median_ms': float(df_success['ttft_ms'].median()),\n",
    "        'p95_ms': float(df_success['ttft_ms'].quantile(0.95)),\n",
    "        'p99_ms': float(df_success['ttft_ms'].quantile(0.99))\n",
    "    },\n",
    "    'itl': {\n",
    "        'mean_ms': float(df_success['itl_ms'].mean()),\n",
    "        'median_ms': float(df_success['itl_ms'].median()),\n",
    "        'p95_ms': float(df_success['itl_ms'].quantile(0.95))\n",
    "    },\n",
    "    'throughput': {\n",
    "        'mean_tokens_per_sec': float(df_success['throughput_tokens_per_sec'].mean()),\n",
    "        'total_tokens': int(df_success['tokens_generated'].sum())\n",
    "    }\n",
    "}\n",
    "\n",
    "if df_telem is not None and len(df_telem) > 0:\n",
    "    summary['gpu'] = {\n",
    "        'peak_vram_gb': float(df_telem['memory_used_mb'].max() / 1024),\n",
    "        'avg_utilization_percent': float(df_telem['gpu_utilization_percent'].mean()),\n",
    "        'peak_temperature_c': float(df_telem['temperature_c'].max()),\n",
    "        'avg_power_w': float(df_telem['power_draw_w'].mean())\n",
    "    }\n",
    "\n",
    "# Save summary\n",
    "summary_path = RESULT_FILE.parent / RESULT_FILE.name.replace('_detailed.json', '_analysis_summary.json')\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"✓ Summary exported to {summary_path.name}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
